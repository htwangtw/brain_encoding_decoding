{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477a41b6",
   "metadata": {},
   "source": [
    "# Brain decoding with SVM\n",
    "\n",
    "## Support vector machines\n",
    "We are going to train a support vector machine (SVM) classifier for brain decoding on the Haxby dataset. SVM is often successful in high dimensional spaces, and it is a popular technique in neuroimaging.\n",
    "\n",
    "In the SVM algorithm, we plot each data item as a point in N-dimensional space that N depends on the number of features that distinctly classify the data points (e.g. when the number of features is 3 the hyperplane becomes a two-dimensional plane.). The objective here is finding a hyperplane (decision boundaries that help classify the data points) with the maximum margin (i.e the maximum distance between data points of both classes). Data points falling on either side of the hyperplane can be attributed to different classes.\n",
    "\n",
    "The scikit-learn [documentation](https://scikit-learn.org/stable/modules/svm.html) contains a detailed description of different variants of SVM, as well as example of applications with simple datasets.\n",
    "\n",
    "## Getting the data\n",
    "We are going to download the dataset from Haxby and colleagues (2001) {cite:p}`Haxby2001-vt`. You can check section {ref}`haxby-dataset` for more details on that dataset. Here we are going to quickly download it, and prepare it for machine learning applications with a set of predictive variable, the brain time series `X`, and a dependent variable, the annotation on cognition `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17005bfb",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/nilearn/datasets/__init__.py:86: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nilearn import datasets\n",
    "# We are fetching the data for subject 4\n",
    "data_dir = os.path.join('..', 'data')\n",
    "sub_no = 4\n",
    "haxby_dataset = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)\n",
    "func_file = haxby_dataset.func[0]\n",
    "\n",
    "# mask the data\n",
    "from nilearn.input_data import NiftiMasker\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_filename, standardize=True, detrend=True)\n",
    "X = masker.fit_transform(func_file)\n",
    "\n",
    "# cognitive annotations\n",
    "import pandas as pd\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "y = behavioral['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9c721",
   "metadata": {},
   "source": [
    "Let's check the size of `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fde2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rest' 'face' 'chair' 'scissors' 'shoe' 'scrambledpix' 'house' 'cat'\n",
      " 'bottle']\n",
      "(1452,)\n",
      "(1452, 675)\n"
     ]
    }
   ],
   "source": [
    "categories = y.unique()\n",
    "print(categories)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26554ed",
   "metadata": {},
   "source": [
    "So we have 1452 time points, with one cognitive annotations each, and for each time point we have recordings of fMRI activity across 675 voxels. We can also see that the cognitive annotations span 9 different categories.\n",
    "\n",
    "Before we carry on with the brain decoding, we first need to convert the cognitive annotations to some numeric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5529f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "# Encoding the string to numerical values\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "y = y.ravel()\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35556e75",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "We are going to start by splitting our dataset between train and test. We will keep 20% of the time points as test, and then set up a 10 fold cross validation for training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494dfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)   \n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits = 10, random_state = 0, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39677759",
   "metadata": {},
   "source": [
    "Now we can initialize a SVM classifier, and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c90a89c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'report'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2308/3372186387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_function_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'report'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svm = SVC(decision_function_shape = 'ovo', random_state = 0, kernel='linear')\n",
    "model_svm.fit(X_train, y_train)\n",
    "model_svm.report"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   14,
   27,
   47,
   50,
   55,
   59,
   67,
   71,
   78,
   81
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}